{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c985a-395a-470a-919d-bbfca010f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load California Housing dataset (Regression)\n",
    "print(\"Loading California Housing dataset...\")\n",
    "X_reg, y_reg = fetch_california_housing(return_X_y=True)\n",
    "X_reg = X_reg.astype('float32')\n",
    "y_reg = y_reg.astype('float32')\n",
    "\n",
    "# Split the regression dataset\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "del X_reg\n",
    "del y_reg\n",
    "\n",
    "# Load MNIST dataset\n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X_mnist, y_mnist = mnist.data.astype('float32') / 255.0, mnist.target.astype('int')\n",
    "\n",
    "X_mnist = np.array(X_mnist)\n",
    "y_mnist = np.array(y_mnist)\n",
    "\n",
    "# Split the MNIST dataset\n",
    "X_train_mnist, X_test_mnist, y_train_mnist, y_test_mnist = train_test_split(X_mnist, y_mnist, test_size=0.2, random_state=42)\n",
    "\n",
    "del X_mnist\n",
    "del y_mnist\n",
    "\n",
    "# Standardize the regression data\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# Standardize the MNIST data\n",
    "scaler_mnist = StandardScaler()\n",
    "X_train_mnist = scaler_mnist.fit_transform(X_train_mnist.reshape(-1, 28 * 28))\n",
    "X_test_mnist = scaler_mnist.transform(X_test_mnist.reshape(-1, 28 * 28))\n",
    "\n",
    "# Create a directory to save the models and weights\n",
    "model_save_dir = \"/content/drive/MyDrive/model_hot_swapping/saved_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "num_written_rows = 0\n",
    "\n",
    "# Create and open a CSV file to store results\n",
    "csv_file_path = \"<csv_path>\"\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Dataset Type', 'Hidden Layers', 'Neurons per Hidden Layer', 'Batch Size', 'Epochs', 'Test Accuracy / MSE Score', 'Inference Time (s)', 'Test Loss', 'Model Name'])\n",
    "else:\n",
    "    with open(csv_file_path, 'r') as csv_file:\n",
    "        # Determine the number of rows already written\n",
    "        num_written_rows = sum(1 for _ in csv_file) - 1  # Subtract header row\n",
    "\n",
    "iteration = 1\n",
    "all_iters = 2 * 10 * 3 * 4 * 3\n",
    "\n",
    "# Iterate through different dataset types\n",
    "for dataset_type, X_train, X_test, y_train, y_test in [('Regression', X_train_reg, X_test_reg, y_train_reg, y_test_reg),\n",
    "                                                       ('MNIST', X_train_mnist, X_test_mnist, y_train_mnist, y_test_mnist)]:\n",
    "    # Iterate through different numbers of layers\n",
    "    for num_layers in range(1, 11):\n",
    "        # Iterate through different numbers of neurons per hidden layer\n",
    "        for neurons_per_layer in [32, 64, 128]:\n",
    "            # Iterate through different batch sizes\n",
    "            for batch_size in [32, 64, 128, 256]:\n",
    "                # Iterate through different epoch sizes\n",
    "                for epochs in [1, 3, 5]:\n",
    "                    print(f\"{iteration} / {all_iters}\")\n",
    "                    iteration += 1\n",
    "                    if num_written_rows > 0:\n",
    "                        # Skip configurations already written to CSV\n",
    "                        num_written_rows -= 1\n",
    "                        continue\n",
    "                    print(f\"\\nTraining MLP on {dataset_type} with {num_layers} layers, {neurons_per_layer} neurons/layer, batch size {batch_size}, {epochs} epochs.\")\n",
    "\n",
    "                    # Build the MLP model\n",
    "                    model = models.Sequential()\n",
    "                    model.add(layers.Flatten(input_shape=(X_train.shape[1],)))  # Input layer\n",
    "\n",
    "                    for _ in range(num_layers):\n",
    "                        model.add(layers.Dense(neurons_per_layer, activation='leaky_relu'))  # Hidden layers\n",
    "\n",
    "                    # Determine the number of classes based on the dataset type\n",
    "                    if dataset_type == 'Regression':\n",
    "                        num_classes = 1  # For regression\n",
    "                        output_activation = 'linear'\n",
    "                        loss_function = 'mean_squared_error'\n",
    "                        optimizer = tf.keras.optimizers.Adam()\n",
    "                        metric = 'mse'\n",
    "                    else:\n",
    "                        num_classes = 10  # For multi-class classification (MNIST)\n",
    "                        output_activation = 'softmax'\n",
    "                        loss_function = 'sparse_categorical_crossentropy'\n",
    "                        optimizer = tf.keras.optimizers.Adam()\n",
    "                        metric = 'accuracy'\n",
    "\n",
    "                    # Output layer\n",
    "                    model.add(layers.Dense(num_classes, activation=output_activation))\n",
    "\n",
    "                    model.compile(optimizer=optimizer,\n",
    "                                  loss=loss_function,\n",
    "                                  metrics=[metric])\n",
    "\n",
    "                    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "                    # Evaluate the model\n",
    "                    start_time = time.time()\n",
    "                    test_loss, test_accuracy_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "                    inference_time = time.time() - start_time\n",
    "\n",
    "                    # Generate a unique name for the model based on configuration\n",
    "                    model_name = f\"model_{dataset_type.lower()}_layers{num_layers}_neurons{neurons_per_layer}_batch{batch_size}_epochs{epochs}.h5\"\n",
    "                    model_path = os.path.join(model_save_dir, model_name)\n",
    "\n",
    "                    # Save both architecture and weights\n",
    "                    model.save(model_path)\n",
    "\n",
    "                    # Write results, configurations, and model name to CSV file\n",
    "                    with open(csv_file_path, 'a', newline='') as csv_file:\n",
    "                        csv_writer = csv.writer(csv_file)\n",
    "                        csv_writer.writerow([dataset_type, num_layers, neurons_per_layer, batch_size, epochs, test_accuracy_mse, inference_time, test_loss, model_name])\n",
    "\n",
    "                    print(f\"Results and models saved to {csv_file_path} and {model_save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
